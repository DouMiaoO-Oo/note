1.SVM的原理是什么？
SVM是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。（间隔最大是它有别于感知机）
（1）当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；
（2）当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；
（3）当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。
注：以上各SVM的数学推导应该熟悉：硬间隔最大化（几何间隔）---学习的对偶问题---软间隔最大化（引入松弛变量）---非线性支持向量机（核技巧）。

 2.SVM为什么采用间隔最大化？

当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果对未知实例的泛化能力最强。



 3.为什么要将求解SVM的原始问题转换为其对偶问题？
一、是对偶问题往往更易求解,（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。）
(a)目前处理的模型严重依赖于数据集的维度d，如果维度d太高就会严重提升运算时间；
(b)对偶问题把SVM从依赖d个维度转变到依赖N个数据点，最后计算时只有支持向量有意义，所以计算量比N小很多。

4.SVM相比于LR有什么优缺点？
SVM相对于LR更适合小样本，因为SVM生成的分类超平面包含N个样本。
SVM相对于LR更适合处理高维数据，因为数据$x \in R^n$ 的维度$n$很大的时候，需要学习的参数$w \in R^n$同样也变多，模型容易过拟合。而SVM需要学习的参数是对偶问题中的$N$个$\alpha$, 当样本个数$N$不大的时候需要学习的参数较少。

二、自然引入核函数，进而推广到非线性分类问题。

