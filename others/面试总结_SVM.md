## 1.SVM的原理是什么？
SVM是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。（间隔最大是它有别于感知机）
（1）当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；
（2）当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；
（3）当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。
注：以上各SVM的数学推导应该熟悉：硬间隔最大化（几何间隔）---学习的对偶问题---软间隔最大化（引入松弛变量）---非线性支持向量机（核技巧）。

##  2.SVM为什么采用间隔最大化？

当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果对未知实例的泛化能力最强。

**能不能最小化数据点到分隔超平面的平均距离来求最佳直线呢？**--出自《机器学习实战》的问题。这样也会面临无穷多个分离超平面，可能很多超平面都有相同的平均距离。此时的损失函数是：

$\frac{1}{m}\sum_{i=1}^{m}\dfrac{y_i(\tilde w \tilde x_i)}{\begin{Vmatrix}\tilde w\end{Vmatrix}}$

**（待补充：怎么证明此时这个不是严格凸的）**

##  3.为什么要将求解SVM的原始问题转换为其对偶问题？
一、是对偶问题往往更易求解,（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。）
(a)目前处理的模型严重依赖于数据集的维度d，如果维度d太高就会严重提升运算时间；
(b)对偶问题把SVM从依赖d个维度转变到依赖N个数据点，最后计算时只有支持向量有意义，所以计算量比N小很多。

4.SVM相比于LR有什么优缺点？
SVM相对于LR更适合小样本，因为SVM生成的分类超平面包含N个样本。
SVM相对于LR更适合处理高维数据，因为数据$x \in R^n$ 的维度$n$很大的时候，需要学习的参数$w \in R^n$同样也变多，模型容易过拟合。而SVM需要学习的参数是对偶问题中的$N$个$\alpha$, 当样本个数$N$不大的时候需要学习的参数较少。

二、自然引入核函数，进而推广到非线性分类问题。



## 为什么SVM要引入核函数？

当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。

引入映射后的对偶问题：

$ \begin{array}{c}{\min _{\lambda} \quad \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \phi^{T}\left(x_{i}\right) \phi\left(x_{j}\right)-\sum_{i=1}^{N} \lambda_{i}} \\ {\text {s.t. } \sum_{i=1}^{N} \lambda_{i} y_{i}=0} \\ {0 \leq \lambda_{i} \leq C, \quad i=1,2, \ldots, N}\end{array}$

在学习预测中，只定义核函数K(x,y)，而不是显式的定义映射函数ϕ。因为特征空间维数可能很高，甚至可能是无穷维，因此直接计算ϕ(x)·ϕ(y)是比较困难的。相反，直接计算K(x,y)比较容易（即直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果）。

核函数的定义：K(x,y)=<ϕ(x),ϕ(y)>，即在特征空间的内积等于它们在原始样本空间中通过核函数K计算的结果。

除了 SVM 之外，任何将计算表示为数据点的内积的方法，都可以使用核方法进行非线性扩展。

lyw补充：可以预先用核函数计算出Gram矩阵，加快之后的训练，没有必要从 $\phi$ 映射做起。

可能可以参考的资料：https://blog.csdn.net/jiangjieqazwsx/article/details/51418681

## SMO算法能找到全局最优解么？

引入拉格朗日乘子的SVM问题是个凸问题，能够找到全局最优解。SMO也能找到全局最优解。（待完善）



## SVM如何处理多分类问题？

一般有两种做法：一种是直接法，直接在目标函数上修改，将多个分类面的参数求解合并到一个最优化问题里面。看似简单但是计算量却非常的大。

另外一种做法是间接法：对训练器进行组合。其中比较典型的有一对一，和一对多。

一对多，就是对每个类都训练出一个分类器，由svm是二分类，所以将此而分类器的两类设定为目标类为一类，其余类为另外一类。这样针对k个类可以训练出k个分类器，当有一个新的样本来的时候，用这k个分类器来测试，那个分类器的概率高，那么这个样本就属于哪一类。这种方法效果不太好，bias比较高。

svm一对一法（one-vs-one），针对任意两个类训练出一个分类器，如果有k类，一共训练出C(2,k) 个分类器，这样当有一个新的样本要来的时候，用这C(2,k) 个分类器来测试，每当被判定属于某一类的时候，该类就加一，最后票数最多的类别被认定为该样本的类。

## SVM 回归

周志华《机器学习》Sec 6.5

## 参考链接

- 《统计学习方法》
- https://blog.csdn.net/szlcw1/article/details/52259668
- 《机器学习实战》